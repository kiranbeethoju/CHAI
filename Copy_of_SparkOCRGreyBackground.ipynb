{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Copy of SparkOCRGreyBackground.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiranbeethoju/CHAI/blob/master/Copy_of_SparkOCRGreyBackground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cks3B1tWWwtm"
      },
      "source": [
        "# OCR example for recogneze text from table with grey bacground for some cells\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGOaoOo_Wwtu"
      },
      "source": [
        "secret = \"1.9.0-03e6c0181eed815013f38d53583a7380d33cde86\"\n",
        "license = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJleHAiOjE2MTI2MDA1NTUsImlhdCI6MTYxMTMwNDU1NSwidW5pcXVlX2lkIjoiZDg0ZmU0MmMtNWM4Yy0xMWViLTgyMDgtNzIyM2RkN2MyNzY0In0.TaUpYFbDvxgBSAj_9Tahk5hSlumdydAzhWmr-y_EkgDOMNfJF-Puvh9bKL27bhIAC03DxDDryKQqR1OOLw-QDqgVrNOL_s7lAjtDk64M0u91WStxvLLzx2mU9JG6rMJ6Al_z2tQPZY2YzjEk1HFvdwDUgSw0oYgrryHLaA1V9V0M88TWeSD6m5zO8bJxgYlowC7IGjwq7wjoutcUjlp9ZpELX-ydadPEACk7Zt4hLV1SGaAzcSDqiSfzMmEi_CKua2gOITJs7QMZnzpecSLUZwOG4eipT6JP5ypC8EKCkwSjM6sTtFp59E5tV0WIXuc9y0wbRJfN7vxQVQJROFbLGQ\"\n",
        "version = secret.split(\"-\")[0]\n",
        "spark_ocr_jar_path = \"../../target/scala-2.11\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aq5zqIWWwtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e623641-7c84-4e28-bfdd-1d7dab5b6bf2"
      },
      "source": [
        "%%bash\n",
        "if python -c 'import google.colab' &> /dev/null; then\n",
        "    echo \"Run on Google Colab!\"\n",
        "    echo \"Install Open JDK\"\n",
        "    apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "    export JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    export PATH=\"$JAVA_HOME/bin:$PATH\"\n",
        "    java -version\n",
        "fi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run on Google Colab!\n",
            "Install Open JDK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_275\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01)\n",
            "OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oiL22hoHb-7",
        "outputId": "a1506808-3007-4ade-87ce-41ef1659d1dd"
      },
      "source": [
        "!pip install spark-nlp==2.7.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L_ZcVWaWwtw"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "  os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJhIQrs7Wwtx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "outputId": "a1d7c8cf-36de-4f21-f9e3-8ab147002a9e"
      },
      "source": [
        "# install from PYPI using secret\n",
        "%pip install spark-ocr==1.9.0.spark24 --extra-index-url=https://pypi.johnsnowlabs.com/$secret --upgrade"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25hCollecting pyspark==2.4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 79kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.16.2->spark-ocr==1.9.0.spark24) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.16.2->spark-ocr==1.9.0.spark24) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.16.2->spark-ocr==1.9.0.spark24) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.16.2->spark-ocr==1.9.0.spark24) (2.5)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image==0.16.2->spark-ocr==1.9.0.spark24) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein==0.12.0->spark-ocr==1.9.0.spark24) (51.3.3)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2->spark-ocr==1.9.0.spark24) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2->spark-ocr==1.9.0.spark24) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2->spark-ocr==1.9.0.spark24) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2->spark-ocr==1.9.0.spark24) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image==0.16.2->spark-ocr==1.9.0.spark24) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2->spark-ocr==1.9.0.spark24) (1.15.0)\n",
            "Building wheels for collected packages: python-levenshtein, pyspark\n",
            "  Building wheel for python-levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144799 sha256=5ae146d242ddb0aa53090abfb0c8a1ac5969f584ffbb903893199c94b746c0a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130389 sha256=30802a1067fd3707bd3d6dc1adb962284bf497aba52ba0d2e2eb4d6ee395741e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built python-levenshtein pyspark\n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement numpy~=1.19.2, but you'll have numpy 1.17.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: implicits, numpy, py4j, pillow, python-levenshtein, pyspark, spark-ocr\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed implicits-1.0.2 numpy-1.17.4 pillow-6.2.1 py4j-0.10.7 pyspark-2.4.4 python-levenshtein-0.12.0 spark-ocr-1.9.0.spark24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3ZJyYd-Wwtx"
      },
      "source": [
        "# or install from local path\n",
        "#%pip install ../../python/dist/spark-ocr-1.9.0.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3TDkCdQWwty"
      },
      "source": [
        "## Initialization of spark session\n",
        "Need specify path to `spark-ocr-assembly-[version].jar` or `secret`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIiUHK6gYPLg"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "  os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "secret = \"1.9.0-03e6c0181eed815013f38d53583a7380d33cde86\"\n",
        "license = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJleHAiOjE2MTI2MDA1NTUsImlhdCI6MTYxMTMwNDU1NSwidW5pcXVlX2lkIjoiZDg0ZmU0MmMtNWM4Yy0xMWViLTgyMDgtNzIyM2RkN2MyNzY0In0.TaUpYFbDvxgBSAj_9Tahk5hSlumdydAzhWmr-y_EkgDOMNfJF-Puvh9bKL27bhIAC03DxDDryKQqR1OOLw-QDqgVrNOL_s7lAjtDk64M0u91WStxvLLzx2mU9JG6rMJ6Al_z2tQPZY2YzjEk1HFvdwDUgSw0oYgrryHLaA1V9V0M88TWeSD6m5zO8bJxgYlowC7IGjwq7wjoutcUjlp9ZpELX-ydadPEACk7Zt4hLV1SGaAzcSDqiSfzMmEi_CKua2gOITJs7QMZnzpecSLUZwOG4eipT6JP5ypC8EKCkwSjM6sTtFp59E5tV0WIXuc9y0wbRJfN7vxQVQJROFbLGQ\"\n",
        "version = secret.split(\"-\")[0]\n",
        "spark_ocr_jar_path = \"../../target/scala-2.11\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQDdxIglWwty",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "9ad04bac-07a0-4942-81ef-bd910f84052b"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from sparkocr import start\n",
        "if license:\n",
        "    os.environ['JSL_OCR_LICENSE'] = license\n",
        "\n",
        "spark = start(secret=secret, jar_path=spark_ocr_jar_path)\n",
        "spark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark version: 2.4.4\n",
            "Spark NLP version: 2.7.1\n",
            "Spark OCR version: 1.9.0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5538efd15a1a:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark OCR</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f7396a36438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq8ZIe5LWwt0"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKb9CD8rWwt1"
      },
      "source": [
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "from sparkocr.transformers import *\n",
        "from sparkocr.enums import *\n",
        "from sparkocr.utils import display_image\n",
        "from sparkocr.metrics import score\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiU7ko0_HANW",
        "outputId": "b17d21c3-329e-42b1-87fb-e884f3a1c690"
      },
      "source": [
        "%%bash\n",
        "if python -c 'import google.colab' &> /dev/null; then\n",
        "    echo \"Run on Google Colab!\"\n",
        "    echo \"Install Open JDK\"\n",
        "    apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "    export JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "    export PATH=\"$JAVA_HOME/bin:$PATH\"\n",
        "    java -version\n",
        "fi"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run on Google Colab!\n",
            "Install Open JDK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_275\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01)\n",
            "OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-gmd_GJWwt1"
      },
      "source": [
        "## Define OCR transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsNx4MQhWwt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30609be-3591-403b-d996-109ec2d80ef6"
      },
      "source": [
        "# Read binary as image\n",
        "binary_to_image = BinaryToImage()\n",
        "binary_to_image.setInputCol(\"content\")\n",
        "binary_to_image.setOutputCol(\"image\")\n",
        "\n",
        "# Scale image\n",
        "scaler = ImageScaler()\n",
        "scaler.setInputCol(\"image\")\n",
        "scaler.setOutputCol(\"scaled_image\")\n",
        "scaler.setScaleFactor(1.5)\n",
        "\n",
        "# Binarize using adaptive tresholding\n",
        "binarizer = ImageAdaptiveThresholding()\n",
        "binarizer.setInputCol(\"scaled_image\")\n",
        "binarizer.setOutputCol(\"binarized_image\")\n",
        "binarizer.setBlockSize(91)\n",
        "binarizer.setOffset(20)\n",
        "\n",
        "# Apply morphology opening\n",
        "opening = ImageMorphologyOperation()\n",
        "opening.setKernelShape(KernelShape.SQUARE)\n",
        "opening.setOperation(MorphologyOperationType.OPENING)\n",
        "opening.setKernelSize(2)\n",
        "opening.setInputCol(\"binarized_image\")\n",
        "opening.setOutputCol(\"opening_image\")\n",
        "\n",
        "# Remove small objects\n",
        "remove_objects = ImageRemoveObjects()\n",
        "remove_objects.setInputCol(\"binarized_image\")\n",
        "remove_objects.setOutputCol(\"corrected_image\")\n",
        "remove_objects.setMaxSizeObject(1000)\n",
        "remove_objects.setMinSizeObject(None)\n",
        "\n",
        "# Run OCR for each region\n",
        "ocr_corrected = ImageToText()\n",
        "ocr_corrected.setInputCol(\"corrected_image\")\n",
        "ocr_corrected.setOutputCol(\"text_corrected\")\n",
        "ocr_corrected.setPositionsCol(\"positions_corrected\")\n",
        "ocr_corrected.setConfidenceThreshold(70)\n",
        "\n",
        "ocr = ImageToText()\n",
        "ocr.setInputCol(\"image\")\n",
        "ocr.setOutputCol(\"text\")\n",
        "\n",
        "# OCR pipeline\n",
        "pipeline = PipelineModel(stages=[\n",
        "    binary_to_image,\n",
        "    scaler,\n",
        "    binarizer,\n",
        "    opening,\n",
        "    remove_objects,\n",
        "    ocr,\n",
        "    ocr_corrected\n",
        "])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BinaryToImage_672c498c17b7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlKn2dvuWwt3"
      },
      "source": [
        "## Read image with noised background"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p-W-NYtWwt4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e41b163c-ce42-41f0-fa25-f055d72ae7d9"
      },
      "source": [
        "import pkg_resources\n",
        "imagePath = \"pdfs/demo.png\"\n",
        "image_df = spark.read.format(\"binaryFile\").load(imagePath).cache()\n",
        "image_df.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------------+------+--------------------+\n",
            "|                path|   modificationTime|length|             content|\n",
            "+--------------------+-------------------+------+--------------------+\n",
            "|file:/content/pdf...|2021-01-23 02:51:40|431306|[89 50 4E 47 0D 0...|\n",
            "+--------------------+-------------------+------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWLAAGDdIigj",
        "outputId": "b5c884e6-d18d-43a1-a705-132b17733f7d"
      },
      "source": [
        "image_df.printSchema"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.printSchema of DataFrame[path: string, modificationTime: timestamp, length: bigint, content: binary]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DIL-ewsWwt6"
      },
      "source": [
        "## Run OCR pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_nT-aXzWwt7"
      },
      "source": [
        "result = pipeline \\\n",
        ".transform(image_df).cache()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Z8cNYsWwt8"
      },
      "source": [
        "## Results of simple OCR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRNQ-t7bWwt8"
      },
      "source": [
        "print(\"\\n\".join([row.text for row in result.select(\"text\").collect()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsJXenKIWwt9"
      },
      "source": [
        "## Results with preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E1mjaudWwt9"
      },
      "source": [
        "print(\"\\n\".join([row.text_corrected for row in result.select(\"text_corrected\").collect()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4GoSavsWwt-"
      },
      "source": [
        "## Display original and corrected images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEsWBOMgWwt-"
      },
      "source": [
        "for r in result.distinct().collect():\n",
        "    print(\"Original: %s\" % r.path)\n",
        "    display_image(r.image)\n",
        "    print(\"Corrected: %s\" % r.path)\n",
        "    display_image(r.corrected_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ncRHkVEWwt_"
      },
      "source": [
        "## Compute score and compare results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqYGE1rLWwuA"
      },
      "source": [
        "original_text = \"\"\"Question: “What would you do as a result of seeing this label posted on a new car's\n",
        "window? Mark all that apply”.\n",
        "Vertical Horizontal\n",
        "Sample size 233 223\n",
        "Write down the MPG rating(s) of the automobile 55% 57%\n",
        "Write down or record the particular data I was interested in 53% 60%\n",
        "Visit the website for more information 45% 45%\n",
        "Write down the EPA-assigned grade of the automobile 43% X\n",
        "Scan the QR code (2-D barcode) with my smartphone 15% 13%\n",
        "Ignore the label and move on to other available information 14% 7%\n",
        "Other 6% 2%\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk8gOml7WwuA"
      },
      "source": [
        "# detected = \"\\n\".join([row.text for row in result.collect()])\n",
        "# detected_corrected = \"\\n\".join([row.text_corrected for row in result.collect()])\n",
        "\n",
        "# # Compute scores\n",
        "# tesseract_score = score(original_text, detected)\n",
        "# corrected_score = score(original_text, detected_corrected)\n",
        "\n",
        "# print(\"Score for simple Tesseract: {0}\".format(tesseract_score))\n",
        "# print(\"Score Spark NLP: {0}\".format(corrected_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX6umonVagxv"
      },
      "source": [
        "# With PDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2dF-bvudsj3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ad3da0c-f9e8-486a-f695-d94e2094ebe4"
      },
      "source": [
        "!pip install pdf2image"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading https://files.pythonhosted.org/packages/03/62/089030fd16ab3e5c245315d63c80b29250b8f9e4579b5a09306eb7e7539c/pdf2image-1.14.0-py3-none-any.whl\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from pdf2image) (6.2.1)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lf2f7vhbyEI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "39a909c4-29ce-4f7e-f775-78d6abece457"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "from sparkocr.transformers import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "\n",
        "documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\").setCleanupMode(\"shrink\")\n",
        "                   \n",
        "                   \n",
        "                   \n",
        "pdfPath = \"pdfs\"\n",
        "\n",
        "# Read PDF file as binary file\n",
        "df = spark.read.format(\"binaryFile\").load(pdfPath)\n",
        "\n",
        "pdf_to_text = PdfToText() \\\n",
        "    .setInputCol(\"content\") \\\n",
        "    .setOutputCol(\"text\") \\\n",
        "    .setPageNumCol(\"page\") \\\n",
        "    .setSplitPage(False)\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentence_detector = SentenceDetector() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "entity_extractor = TextMatcher() \\\n",
        "    .setInputCols(\"sentence\", \"token\") \\\n",
        "    .setEntities(\"./sparkocr/resources/test-chunks.txt\", ReadAs.TEXT) \\\n",
        "    .setOutputCol(\"entity\")\n",
        "\n",
        "position_finder = PositionFinder() \\\n",
        "    .setInputCols(\"entity\") \\\n",
        "    .setOutputCol(\"coordinates\") \\\n",
        "    .setPageMatrixCol(\"positions\") \\\n",
        "    .setMatchingWindow(10) \\\n",
        "    .setPadding(2)\n",
        "\n",
        "draw = PdfDrawRegions() \\\n",
        "    .setInputRegionsCol(\"coordinates\") \\\n",
        "    .setOutputCol(\"pdf_with_regions\") \\\n",
        "    .setInputCol(\"content\") \\\n",
        "    .setLineWidth(1)\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    pdf_to_text,\n",
        "    document_assembler,\n",
        "    sentence_detector,\n",
        "    tokenizer,\n",
        "    entity_extractor,\n",
        "    position_finder,\n",
        "    draw\n",
        "])\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-9414b1f22ba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msetInputCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"document\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sparknlp/annotator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         super(SentenceDetector, self).__init__(\n\u001b[0;32m-> 1024\u001b[0;31m             classname=\"com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector\")\n\u001b[0m\u001b[1;32m   1025\u001b[0m         self._setDefault(\n\u001b[1;32m   1026\u001b[0m             \u001b[0museAbbreviations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sparknlp/common.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, classname, java_model)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclassname\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjava_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_class_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mjava_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_from_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBUnFdUkbHx7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}